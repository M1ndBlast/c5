{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consignador RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido\n",
    "1. [Importar librerias](#Importar-librerias)\n",
    "2. [Cargar datos](#Cargar-datos)\n",
    "3. [Preparar datos](#Preparar-datos)\n",
    "4. [Modelo](#Modelo)\n",
    "5. [Entrenamiento](#Entrenamiento) (#TODO: agregar grafico de perdida y guardar mejor modelo en cada epoca)\n",
    "6. [Validacion](#Validacion)\n",
    "7. [Prediccion](#Prediccion)\n",
    "8. [Guardar modelo](#Guardar-modelo)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "import utilities as ut\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import AUC, Precision, Recall\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(ut.INCIDENTES_CAMARAS_FILENAME)\n",
    "# convertir el id_camara a int\n",
    "data['id_camara'] = data['id_camara'].astype(int)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar datos de puntos de interes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = '\\\\\\\\C4wadpninv004\\\\ANALISIS II-DGGE\\\\02. SME\\\\GUSTAVO\\\\'\n",
    "# DIR_PATH = '..\\\\data\\\\'\n",
    "FILE_PATH = DIR_PATH + 'B200m_CONSIGNADAS.csv'\n",
    "FILE_PATH = DIR_PATH + 'B200m_CONSIGNADAS-v3.csv'\n",
    "data_interest_points = pd.read_csv(FILE_PATH, sep=',', encoding='latin-1')\n",
    "# Renombrar la columna 'Etiquetas' a 'id_camara'\n",
    "data_interest_points.rename(columns={'id': 'id_camara'}, inplace=True)\n",
    "# Ordenar los datos por la columna 'id_camara'\n",
    "data_interest_points.sort_values(by=['id_camara'], inplace=True)\n",
    "# Resetear los índices\n",
    "data_interest_points.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_interest_points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrupar los puntos de interes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_groups = [['C09-CUARTELES DE LA POLICÍA AUXILIAR','C10-CUARTELES PBI','C11-CUARTELES PGJ','C16-JUZGADOS CIVILES Y PENALES','C20-MINISTERIOR PUBLICOS','C21-MODULOS SSP',],    # P01 - Seguridad\n",
    "\n",
    "['C05-CENTRALES CAMIONERAS','C19-METROBUS','C31-TREN LIGERO','C42-TROLEBUS','C43-TURIBUS','C72-ACCESOS METRO','C81-ESTACIONES DE CABLEBUS',],                                        # P02 - Transporte\n",
    "\n",
    "['C06-CENTROS COMERCIALES','C38-MERCADOS PUBLICOS','C49-TIENDAS DEPARTAMENTALES','C53-CINES','C62-OXXO',],#'C46-ESTABLECIMIENTOS MERCANTILES',],                                    # P03 - Comercio\n",
    "\n",
    "['C22-MONUMENTOS HISTORICOS', 'C57-EVENTOS MASIVOS', 'C73-ATRACTIVOS TURISTICOS', 'C78-ZONAS ARQUEOLOGICAS',],                                                                        # P04 - Turismo\n",
    "\n",
    "['C58-CASAS Y CENTROS DE CULTURA','C59-MUSEOS Y TEATROS','C75-FONOTECAS','C76-FOTOTECA','C77-GALERIAS', 'C26-PLAZAS Y PARQUES',],                                                    # P05 - Cultura\n",
    "\n",
    "['C23-NOTARIAS','C24-OFICINAS DE GOBIERNO','C27-RECLUSORIOS','C30-TESORERIAS','C82-ALCALDIAS', 'C01-BANCOS'],                                                                        # P06 - Gobierno\n",
    "\n",
    "['C15-HOSPITALES','C45-CENTROS DE SALUD Y CLINICAS',],                                                                                                                                # P07 - Hospitales\n",
    "\n",
    "['C44-GUARDERIAS', 'C67-CENTROS PILARES', 'C13-EDUCACION',],#'C13-CAM', 'C13-CPARA EL TRABAJO', 'C13_PREESC', 'C13_PRIM', 'C13_SEC', 'C13_BACH', 'C13_SUP',],                                            # P08 - Escuelas\n",
    "\n",
    "['C33-IGLESIAS Y TEMPLOS',],                                                                                                                                                        # P09 - Iglesias\n",
    "\n",
    "['C51-EDIFICIOS',],]                                                                                                                                    # P10 - Edificios\n",
    "\n",
    "# Cada nueva columna es la suma de las columnas que se encuentran en la lista de agrupaciones con nombre GRUPO-k\n",
    "for i in range(len(IP_groups)):\n",
    "    data_interest_points['P.Interes-'+str(i+1)] = data_interest_points[IP_groups[i]].sum(axis=1)\n",
    "    # # Eliminamos las columnas que ya no se van a utilizar\n",
    "    data_interest_points = data_interest_points.drop(IP_groups[i], axis=1)\n",
    "data_interest_points = data_interest_points.drop([\"C46-ESTABLECIMIENTOS MERCANTILES\"], axis=1)\n",
    "\n",
    "# Estandarizamos los datos\n",
    "scaler = StandardScaler(with_mean=False, with_std=False)\n",
    "data_interest_points[data_interest_points.filter(regex='^P.Interes-\\d+').columns] = scaler.fit_transform(data_interest_points[data_interest_points.filter(regex='^P.Interes-\\d+').columns])\n",
    "# data_interest_points[data_interest_points.filter(regex='^P.Interes-\\d+').columns] = data_interest_points[data_interest_points.filter(regex='^P.Interes-\\d+').columns].copy()\n",
    "data_interest_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = data['fecha_creacion'].dt.strftime('%Y-%m-%d').astype('datetime64[ns]')\n",
    "camaras_fechas_frecuencias = data.groupby(['id_camara', 'date']).size().reset_index(name='n_incidentes')\n",
    "\n",
    "# Convertir la columna de fecha a datetime si aún no lo es\n",
    "camaras_fechas_frecuencias['date'] = pd.to_datetime(camaras_fechas_frecuencias['date'])\n",
    "\n",
    "# Crear un DataFrame con todas las fechas posibles\n",
    "\n",
    "fechas = pd.date_range(start='2021-01-01', end=camaras_fechas_frecuencias['date'].max().date().isoformat())\n",
    "\n",
    "# Obtener los id únicos de camara\n",
    "camaras = camaras_fechas_frecuencias['id_camara'].unique()\n",
    "\n",
    "# Crear un DataFrame con todas las combinaciones posibles de id_camara y fechas\n",
    "df_total = pd.DataFrame(index=pd.MultiIndex.from_product([camaras, fechas], names=['id_camara', 'date'])).reset_index()\n",
    "\n",
    "# Unir el DataFrame total con el DataFrame original\n",
    "data_final = pd.merge(df_total, camaras_fechas_frecuencias, how='left', on=['id_camara', 'date'])\n",
    "\n",
    "# Llenar los valores nulos con 0\n",
    "data_final['n_incidentes'] = data_final['n_incidentes'].replace(np.nan, 0)\n",
    "\n",
    "# Ahora desagregamos la fecha\n",
    "data_final['anio'] = data_final['date'].dt.year\n",
    "data_final['mes'] = data_final['date'].dt.month\n",
    "data_final['dia_semana'] = data_final['date'].dt.dayofweek+1\n",
    "data_final['dia'] = data_final['date'].dt.day\n",
    "# data_final['hora'] = data_final['date'].dt.hour\n",
    "\n",
    "# data_final.drop(['date'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "data_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asignar los puntos de interes a la cada registro de la base de datos de 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorrer los registros del df data y agregar las columnas de P.Interes de acuerdo a la columna id_camara\n",
    "interest_points_columns = data_interest_points.filter(regex='^P.Interes-\\d+').columns\n",
    "data_final[data_interest_points.filter(regex='^P.Interes-\\d+').columns] = 0\n",
    "for i in range(len(data_final)):\n",
    "    print(f\"Procesando registro {i+1} de {len(data_final)}\", end='\\r')\n",
    "    id_camara = data_final['id_camara'].iloc[i]\n",
    "    # Buscar el id_camara en el df data_interest_points\n",
    "    index = data_interest_points[data_interest_points['id_camara'] == id_camara].index\n",
    "    if len(index) > 0:\n",
    "        data_final.loc[i, interest_points_columns] = data_interest_points.loc[index[0], interest_points_columns]#**2\n",
    "\n",
    "# data_final['n_incidentes'] = data_final['n_incidentes'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "data = data_final.copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_split = '2023-01-01'\n",
    "# date_split = '2022-10-01'\n",
    "\n",
    "#Split data into train and test, train will be until 2022 and test will be rest of the data\n",
    "train = data[data['date'] < date_split]\n",
    "test = data[data['date'] >= date_split]\n",
    "\n",
    "#Split train and test into X and y\n",
    "X_train = train.drop(['id_camara', 'n_incidentes', 'date'], axis=1)\n",
    "y_train = train['n_incidentes']\n",
    "X_test = test.drop(['id_camara', 'n_incidentes', 'date'], axis=1)\n",
    "y_test = test['n_incidentes']\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "# Imprimir el porcentaje de train y test respecto al total\n",
    "print(f\"Porcentaje de train: {round(len(train)/len(data)*100, 2)}%\")\n",
    "print(f\"Porcentaje de test: {round(len(test)/len(data)*100, 2)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling para las clases minoritarias\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resumen de la distribución de clases\n",
    "from collections import Counter\n",
    "counter = Counter(y_train)\n",
    "print(counter)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Red Neuronal - Regresion Logistica"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deteccion del ultimo modelo ya existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = '../models/'\n",
    "import os\n",
    "\n",
    "# Find last model id from filename structure: model-{id}-{epoch:03d}-{accuracy:.3f}.h5\n",
    "def find_last_model_id():\n",
    "    last_model_id = 0\n",
    "    for file in os.listdir(MODELS_PATH):\n",
    "        if file.startswith('model-'):\n",
    "            model_id = int(file.split('-')[1])\n",
    "            if model_id > last_model_id:\n",
    "                last_model_id = model_id\n",
    "    return last_model_id\n",
    "print(find_last_model_id())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion del modelo con grafica de perdida y accuracy\n",
    "# Pasos:\n",
    "# 1. Definir el modelo\n",
    "# 2. Entrenar el modelo\n",
    "# 3. Evaluar el modelo\n",
    "# 4. Guardar el modelo\n",
    "# 5. Cargar el modelo\n",
    "# 6. Hacer predicciones con el modelo\n",
    "\n",
    "# Solo toma como el total, los que son 1 y de esos verifica cuantos y_pred son 1\n",
    "def c5_score(y_true, y_pred):\n",
    "\timport tensorflow as tf\n",
    "\ty_true = tf.cast(y_true, tf.float32) # cast means convert\n",
    "\ty_pred = tf.cast(tf.round(y_pred), tf.float32) # round means round to nearest integer\n",
    "\t# probando el valor que debera regresar\n",
    "\ty_pred[y_true != 0] = 1\n",
    "\treturn tf.keras.metrics.binary_accuracy(y_true, y_pred)\n",
    "\n",
    "# 1 Definir el modelo\n",
    "def create_model(input_dim, output_dim, hidden_layers, neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=input_dim, activation='relu'))\n",
    "    for i in range(hidden_layers):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adagrad', metrics=['accuracy', c5_score ])\n",
    "    return model\n",
    "\n",
    "# 2 Entrenar el modelo\n",
    "def train_model(model, X_train, y_train, epochs, batch_size, validation_split, model_id):\n",
    "    # Crear el callback para guardar el modelo\n",
    "    checkpoint = ModelCheckpoint(MODELS_PATH+'model-'+model_id+'-{accuracy:.4f}-{epoch:03d}.h5', verbose=0, monitor='accuracy', save_best_only=True, mode='auto')\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, callbacks=[checkpoint])\n",
    "    return history\n",
    "\n",
    "# 3 Evaluar el modelo\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    _, accuracy = model.evaluate(X_test, y_test) # loss, accuracy\n",
    "    return accuracy\n",
    "\n",
    "# 4 Guardar el mejor modelo (el que tenga el mejor accuracy)\n",
    "def save_model(model, model_id, accuracy, epoch):\n",
    "    model.save(f'{MODELS_PATH}model-{model_id}-{epoch:03d}-{accuracy:.3f}.h5')\n",
    "    \n",
    "# 5 Cargar el modelo\n",
    "def load_model(model_path):\n",
    "    model = load_model(model_path, custom_objects={'c5_score': c5_score})\n",
    "    return model\n",
    "\n",
    "# 6 Hacer predicciones con el modelo\n",
    "def predict(model, X):\n",
    "    predictions = model.predict(X)\n",
    "    return predictions\n",
    "\n",
    "# 7 Graficar la perdida y el accuracy\n",
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Precisión del modelo')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.xlabel('Época')\n",
    "    plt.legend(['Entrenamiento', 'Validación'], loc='upper left')\n",
    "    plt.show()\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Pérdida del modelo')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.xlabel('Época')\n",
    "    plt.legend(['Entrenamiento', 'Validación'], loc='upper left')\n",
    "    plt.show()\n",
    "        \n",
    "# 8 Graficar la matriz de confusión\n",
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Matriz de confusión')\n",
    "    plt.show()\n",
    "        \n",
    "# 9 Graficar la curva ROC (Receiver Operating Characteristic), es decir, la curva de la sensibilidad (recall) vs la especificidad\n",
    "def plot_roc_curve(y_test, y_pred):\n",
    "    from sklearn.metrics import roc_curve\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.title('Curva ROC')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 1 Definir el modelo\n",
    "model = create_model(input_dim=X_train.shape[1], output_dim=1, hidden_layers=2, neurons=32)\n",
    "model.summary()\n",
    "print(\"\")\n",
    "model_id = find_last_model_id() + 1\n",
    "\n",
    "# 2 Entrenar el modelo\n",
    "history = train_model(model, X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, model_id=str(model_id))\n",
    "\n",
    "# 3 Evaluar el modelo\n",
    "accuracy = evaluate_model(model, X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# 4 Guardar el mejor modelo (el que tenga el mejor accuracy)\n",
    "save_model(model, model_id, accuracy, len(history.history['accuracy']))\n",
    "\n",
    "# 5 Cargar el modelo\n",
    "# model = load_model(f'{MODELS_PATH}model-{model_id}-{len(history.history[\"accuracy\"]):03d}-{accuracy:.3f}.h5')\n",
    "\n",
    "# 6 Hacer predicciones con el modelo\n",
    "y_pred = predict(model, X_test)\n",
    "y_pred = np.round(y_pred).astype(int).reshape(1, -1)[0]\n",
    "print(y_pred)\n",
    "\n",
    "# 7 Graficar la perdida y el accuracy\n",
    "plot_history(history)\n",
    "\n",
    "# 8 Graficar la matriz de confusión\n",
    "plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 9 Graficar la curva ROC (Receiver Operating Characteristic), es decir, la curva de la sensibilidad (recall) vs la especificidad\n",
    "plot_roc_curve(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crear red neuronal de regresión logística\n",
    "# # Definir el modelo\n",
    "# model = Sequential()\n",
    "\n",
    "# # Añadir la capa de entrada y una capa oculta de 64 neuronas\n",
    "# model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "# # Añadir la capa de entrada y una capa oculta de 32 neuronas\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# # Añadir la capa de entrada y una capa oculta de 32 neuronas\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# # Añadir otra capa oculta\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# # Añadir la capa de salida binaria, 0 o 1, no numeros decimales\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# from keras.optimizers import Adam, Adagrad, SGD\n",
    "\n",
    "# # Para condicion de paro en convergencia realizar el siguiente paso\n",
    "# # En la linea \n",
    "\n",
    "# # Compilar el modelo\n",
    "# model.compile(optimizer=Adagrad(learning_rate=0.01),\n",
    "#               loss='binary_crossentropy', \n",
    "#               metrics=['accuracy', AUC(name='auc')])\n",
    "\n",
    "\n",
    "# # Crear el callback para guardar el modelo\n",
    "# ID_NEW_MODEL = find_last_model_id() + 1\n",
    "# checkpoint = ModelCheckpoint(MODELS_PATH+'model-'+str(ID_NEW_MODEL)+'-{accuracy:.4f}-{epoch:03d}.h5', verbose=0, monitor='accuracy', save_best_only=True, mode='auto')\n",
    "# # Entrenar el modelo\n",
    "# # model.fit(data.drop(['id_camara','date','n_incidentes'], axis=1), data['n_incidentes'], epochs=100, callbacks=[checkpoint], validation_split=0.2)\n",
    "# model.fit(X_train, y_train, epochs=100, callbacks=[checkpoint], validation_split=0.2, use_multiprocessing=True, workers=4)\n",
    "\n",
    "# # Guardar el modelo\n",
    "# model.save(f'{MODELS_PATH}model-{ID_NEW_MODEL}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar el modelo (Recuerda importar la librería: from keras.models import load_model)\n",
    "# model = load_model(f'{MODELS_PATH}model-{ID_NEW_MODEL}.h5')\n",
    "model = load_model(f'{MODELS_PATH}model-2-0.6255-091.h5')\n",
    "model.evaluate(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir\n",
    "probs = model.predict(X_test)\n",
    "probs = probs.reshape((probs.shape[0],))\n",
    "labels = (probs >= 0.5).astype(int)\n",
    "probs, labels\n",
    "# np.unique(probs, return_counts=True)\n",
    "# predictions = np.round(predictions)\n",
    "# predictions = predictions.astype(int)\n",
    "# np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el accuracy\n",
    "print(\"Accuracy:\",metrics.classification_report(y_test, labels))\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "print(metrics.confusion_matrix(y_test, labels, labels=y_test.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar dos graficas, una con los datos de labels y otra con los datos de y_test\n",
    "# El eje x es probs (probabilidades) y el eje y es el redondeo de las probabilidades, el color sera si son verdaderamente 0 o 1\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(probs, labels, c=y_test)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
